# RAID-LINUX
---
## üí™ Challenge

Dans un premier temps, ex√©cute les diff√©rentes actions de cette qu√™te :

- Cr√©er un RAID 1 avec 2 disques
- Reconstruire le RAID apr√®s une simulation de panne

Une fois que tout est fonctionnel, r√©p√©tez l'op√©ration, mais cette fois-ci avec un autre type de RAID, comme le RAID 5 par exemple.

## üßê Crit√®res d'acceptation

- Cr√©ation d'un RAID 1 fonctionnel
- Cr√©ation d'un autre type de RAID fonctionnel
---


# Solution : Configuration RAID sous Linux

## √âtape 1 : Installation d'Ubuntu sur VirtualBox

J'ai cr√©√© une machine virtuelle avec un disque dur de 20 Go pour l'installation d'Ubuntu.
La configuration du disque de 20 Go dans VirtualBox est comme ci-dessous :

![C1_HDD120](https://github.com/user-attachments/assets/f3acab73-f985-43b2-884b-c5b4db708a34)


## √âtape 2 : Ajout des disques suppl√©mentaires

J'ai ajout√© trois disques suppl√©mentaires de 5 Go chacun pour les manipulations sur le RAID. L'option "Branchable √† chaud" ou "Hot-Pluggable"(version anglaise de virtualbox) a √©t√© activ√©e pour permettre la d√©connexion √† chaud des disques.

La configuration des disques de 5 Go dans VirtualBox est comme ci-dessous :

![C1_HDD25G](https://github.com/user-attachments/assets/86b92b55-d6cd-4bff-942c-741e0b88a062)

Pour ajouter un disque il y a l'icone avec un plus dessus qu'on peut apercevoir en bas du cadre "Unit√©s de stockage". Configurer comme sur la capture d'√©cran le nouveau disque et r√©p√©ter l'op√©ration deux fois.   
*Note*: Les disques sont correctement configur√©s pour √™tre branchables √† chaud, ce qui est essentiel pour la simulation de pannes lors de la configuration du RAID.

## √âtape 3 : Cr√©ation du RAID 1

### V√©rification des disques disponibles

Apr√®s le red√©marrage de la VM, j'ai v√©rifi√© que les disques suppl√©mentaires √©taient bien reconnus par le syst√®me. Pour cela, j'ai utilis√© la commande `lsblk` qui liste tous les p√©riph√©riques de stockage disponibles sur le syst√®me.

Voici la sortie de la commande :

![lsblk](https://github.com/user-attachments/assets/dc0d6e0a-5c80-44e7-af61-77568db53a6e)


Comme on peut le voir dans la capture d'√©cran, les disques suivants sont disponibles :
- `/dev/sda` : Le disque principal de 20 Go utilis√© pour l'installation de l'OS.
- `/dev/sdb` : Le premier disque suppl√©mentaire de 5 Go.
- `/dev/sdc` : Le deuxi√®me disque suppl√©mentaire de 5 Go.
- `/dev/sdd` : Le troisi√®me disque suppl√©mentaire de 5 Go.

Ces disques sont d√©sormais pr√™ts √† √™tre utilis√©s pour la cr√©ation du RAID 1.

### Partitionnement du disque `/dev/sdb` pour le RAID 1

Pour pr√©parer le disque `/dev/sdb` √† √™tre utilis√© dans la configuration RAID 1, j'ai utilis√© l'outil `fdisk`. Voici les √©tapes que j'ai suivies :

1. **Lancer `fdisk`** :
- J'ai lanc√© `fdisk` avec la commande suivante pour modifier les partitions du disque `/dev/sdb` :
```bash
sudo fdisk /dev/sdb
```

2. **Cr√©ation d'une nouvelle partition** :
- Apr√®s avoir lanc√© `fdisk`, la premi√®re √©tape consiste √† cr√©er une nouvelle partition primaire :
     - **Commande `n`** : J'ai s√©lectionn√© `n` pour cr√©er une nouvelle partition.
     - **Choix de la partition primaire `p`** : J'ai ensuite s√©lectionn√© `p` pour indiquer qu'il s'agirait d'une partition primaire.
     - **Num√©ro de partition** : J'ai accept√© la valeur par d√©faut `1` pour le num√©ro de partition.
     - **Secteurs de d√©but et de fin** : J'ai accept√© les valeurs par d√©faut pour que la partition utilise tout l'espace disponible sur le disque.

3. **D√©finir le type de partition** :
- Pour configurer cette partition √† √™tre utilis√©e dans un RAID, j'ai chang√© le type de la partition :
     - **Commande `t`** : J'ai utilis√© `t` pour changer le type de la partition.
     - **Type `fd`** : J'ai saisi `fd` pour sp√©cifier que cette partition serait utilis√©e pour un RAID Linux (Linux RAID autodetect).

4. **Enregistrement des modifications** :
- Enfin, j'ai sauvegard√© les modifications et quitt√© `fdisk` :
  - **Commande `w`** : J'ai utilis√© `w` pour √©crire les modifications sur le disque et quitter l'outil `fdisk`.

Voici une capture d'√©cran montrant l'ensemble du processus pour le disque `/dev/sdb` :


![C4_fdisksdb](https://github.com/user-attachments/assets/2322992c-b983-4824-a9b1-b21774d42408)


*Note*: Les m√™mes √©tapes ont √©t√© suivies pour le disque `/dev/sdc` afin de le pr√©parer pour le RAID 1.

Voici une capture d'√©cran montrant l'ensemble du processus pour le disque `/dev/sdc` :   

![C5_fdisksdc](https://github.com/user-attachments/assets/13c1148d-fe59-4735-b95d-cc64e8972886)    

### Installation de `mdadm`(en tant que superutilisateur)

Pour cr√©er et g√©rer le RAID sous Linux, j'ai d'abord install√© l'outil `mdadm`, qui n'√©tait pas pr√©install√© sur le syst√®me Ubuntu.
**mdadm** est un utilitaire Linux utilis√© pour g√©rer les ensembles RAID logiciels (Redundant Array of Independent Disks) pour le stockage de donn√©es.

1. **Mise √† jour des paquets** :
- J'ai mis √† jour la liste des paquets disponibles avec la commande suivante :
```bash
apt update
```

2. **Installation de mdadm** :
   
- Ensuite, j'ai install√© mdadm avec la commande suivante :

```bash
apt install mdadm -y
```

3. **V√©rification de l'installation** :

- Puis, j'ai v√©rifi√© que mdadm est bien install√© avec la commande suivante :

```bash
mdadm --version
```

- **Int√©gration des captures d'√©cran** : J'ai inclus les captures d'√©cran aux moments o√π elles sont pertinentes dans le processus, c'est-√†-dire apr√®s la mise √† jour des paquets et apr√®s l'installation de `mdadm` et la v√©rification de sa version. Les voici :

![C6_updandinstalmdadm1](https://github.com/user-attachments/assets/160db022-f807-46ec-b29c-21c37017a3d8)

![C7_endinstandvers](https://github.com/user-attachments/assets/35420f1c-5534-4792-9ada-9144d773c278)

### Cr√©ation du RAID 1 avec `mdadm`

J'ai utilis√© l'outil `mdadm` pour cr√©er un RAID 1 avec les partitions `/dev/sdb1` et `/dev/sdc1`. La commande suivante a √©t√© utilis√©e :

```bash
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
```

Cette commande a configur√© un RAID 1 sur le p√©riph√©rique /dev/md0, utilisant les deux partitions sp√©cifi√©es.   

V√©rification de l'√©tat du RAID 1

Pour v√©rifier que le RAID 1 a √©t√© correctement cr√©√© et qu'il est en √©tat "clean", j'ai utilis√© la commande suivante :


```bash
cat /proc/mdstat
```

- **Capture d'√©cran** :

![C8_raid1crandver](https://github.com/user-attachments/assets/8f134d0c-1eab-49c4-acdb-464f7d3a3c98)    

**Explication des r√©sultats** :

Le RAID 1 est bien actif (md0 : active raid1).
Les deux partitions, /dev/sdc1 et /dev/sdb1, sont correctement int√©gr√©es au RAID 1.
Le statut [UU] indique que les deux disques sont synchronis√©s et en bon √©tat (Up).
Le processus de resynchronisation (resync) est visible, et une fois termin√©, le RAID sera compl√®tement op√©rationnel.

**D√©tails du RAID 1** :    

Pour obtenir plus de d√©tails sur le RAID que je viens de cr√©er, j'ai utilis√© la commande suivante :   

```bash
mdadm --detail /dev/md0
```

![C9_detailraid](https://github.com/user-attachments/assets/030972c8-4053-4c3b-abcc-19def7d42715)


**Explication des r√©sultats affich√©s** :  

- Version : Le RAID est configur√© avec la version 1.2 du superblock de mdadm.
- Creation Time : Le RAID a √©t√© cr√©√© √† la date et heure indiqu√©es.
- Raid Level : Le RAID est de type 1 (mirroring).
- Array Size : La taille totale du RAID est de 4.99 GiB, correspondant √† la -taille d'un des disques (car dans un RAID 1, la taille totale est √©gale √† la taille du plus petit disque).
- State : Le RAID est en √©tat "clean", signifiant qu'il est op√©rationnel et que les disques sont synchronis√©s.
- Active Devices : Les deux disques sont actifs et synchronis√©s (active sync pour /dev/sdb1 et /dev/sdc1).
- UUID : Un UUID unique a √©t√© attribu√© au RAID pour son identification

**Conclusion**
Le RAID 1 a √©t√© cr√©√© avec succ√®s, et les deux disques sont correctement synchronis√©s. Le RAID est maintenant op√©rationnel et pr√™t √† √™tre utilis√© pour la sauvegarde des donn√©es.

Prochaine √©tape : Le RAID 1 √©tant fonctionnel, nous passerons √† son formatage et √† son montage sur le syst√®me de fichiers.

### Formatage et montage du RAID 1

#### Formatage du RAID 1

Pour pouvoir utiliser le RAID 1, j'ai d'abord format√© le p√©riph√©rique RAID `/dev/md0` avec le syst√®me de fichiers `ext4` :

```bash
mkfs.ext4 /dev/md0 -L "RAID1_Data"
```

Explication :

- mkfs.ext4 : Cette commande formate le p√©riph√©rique avec le syst√®me de fichiers ext4.
- /dev/md0 : C'est le p√©riph√©rique RAID que nous venons de cr√©er.
- -L "RAID1_Data" : Ce param√®tre attribue un label (nom) au syst√®me de fichiers, ici "RAID1_Data".

#### Cr√©ation d'un point de montage

Commande pour cr√©er le r√©pertoire de montage :

```bash
mkdir /mnt/raid1
```
Explication :

- mkdir : Cette commande cr√©e un nouveau r√©pertoire.
- /mnt/raid1 : C'est le chemin du r√©pertoire que nous cr√©ons pour monter le RAID.

#### Montage du RAID 1

```bash
mount /dev/md0 /mnt/raid1
```
Explication :

- mount : Cette commande monte le syst√®me de fichiers sur un r√©pertoire.
- /dev/md0 : Le p√©riph√©rique RAID que nous avons format√©.
- /mnt/raid1 : Le r√©pertoire o√π nous montons le RAID.

#### V√©rification du montage

```bash
mount /dev/md0 /mnt/raid1
```
Explication :

- df -h : Cette commande affiche la liste des syst√®mes de fichiers mont√©s avec leur espace utilis√© et disponible en format lisible (human-readable).

Voici une copie d'√©cran de toutes les commandes utilis√©es :

![C10_montageraid1](https://github.com/user-attachments/assets/ff51c257-ef95-4fc7-9eb2-bab66fc46423)  

## √âtape 3 : Simulation d'une panne et reconstruction du RAID 1

Dans cette √©tape, nous allons simuler une panne d'un des disques du RAID 1 et observer comment le RAID r√©agit. Ensuite, nous allons reconstruire le RAID en rempla√ßant le disque d√©faillant.

#### Simuler la panne d'un disque

Nous allons marquer l'un des disques comme d√©faillant pour simuler une panne.

Commande pour marquer un disque comme d√©faillant :

```bash
mdadm --manage /dev/md0 --fail /dev/sdb1
```
**Explication** :

- mdadm --manage : Cette commande permet de g√©rer un ensemble RAID existant.
- --fail /dev/sdb1 : Marque la partition /dev/sdb1 comme d√©faillante dans l'ensemble RAID /dev/md0.


V√©rification de l'√©tat apr√®s la panne en tapant la commande :

```bash
cat /proc/mdstat
```
Voici la r√©ponse √† la commande : 

```bash
root@karim-VirtualBox:/home/karim# cat /proc/mdstat
Personalities : [raid1]
md0 : active raid1 sdc1[1] sdb1[0](F)
      5236736 blocks super 1.2 [2/1] [_U]

unused devices: <none>
root@karim-VirtualBox:/home/karim# 
```
**Explication r√©ponse** :   

md0 : active raid1 sdc1[1] sdb1[0](F)

- sdb1[0](F) indique que la partition /dev/sdb1 est consid√©r√©e comme d√©faillante (F pour "faulty").
- Le RAID continue de fonctionner en mode d√©grad√© avec la seule partition active restante, /dev/sdc1.

J'ai ensuite retir√© le disque d√©faillant du RAID avec la commande suivante : 

```bash
mdadm --manage /dev/md0 --remove /dev/sdb1
```
Voici la r√©ponse √† la commande : 

```bash
root@karim-VirtualBox:/home/karim# mdadm --manage /dev/md0 --remove /dev/sdb1
mdadm: hot removed /dev/sdb1 from /dev/md0
root@karim-VirtualBox:/home/karim# 
```
R√©sultat : Le disque /dev/sdb1 a √©t√© retir√© du RAID, comme confirm√© par le message hot removed /dev/sdb1 from /dev/md0.

Ajouter un nouveau disque au RAID
Pour remplacer le disque d√©faillant, j'ai ajout√© la m√™me partition au RAID apr√®s l'avoir "r√©par√©e" :

```bash
mdadm --manage /dev/md0 --add /dev/sdb1
```
R√©sultat : Le disque /dev/sdb1 a √©t√© ajout√© au RAID, et la reconstruction a commenc√© automatiquement. Voici la sortie de cat /proc/mdstat montrant l'√©tat de la reconstruction :  

```bash
root@karim-VirtualBox:/home/karim# cat /proc/mdstat
Personalities : [raid1]
md0 : active raid1 sdb1[2] sdc1[1]
      5236736 blocks super 1.2 [2/1] [_U]
      [========>............]  recovery = 44.5% (2335552/5236736) finish=0.5min speed=83412K/sec

unused devices: <none>
root@karim-VirtualBox:/home/karim# 
```

V√©rification de la reconstruction
La reconstruction du RAID est en cours avec un taux d'avancement de 44,5 %. Une fois la reconstruction termin√©e, le RAID sera de nouveau en √©tat "clean".  

Capture d'√©cran des commandes et sorties de commande :

![C12_failandrepair](https://github.com/user-attachments/assets/f7ad8c1f-0244-4121-acfc-3c86d4b5c1dd)   


**V√©rification finale** : La reconstruction du RAID est termin√©e, et le RAID est de nouveau en √©tat "clean", avec les deux disques actifs et synchronis√©s ([UU]).

### Cr√©ation du RAID 5

#### D√©montage et arr√™t du RAID 1

Pour lib√©rer les partitions utilis√©es dans le RAID 1 et les utiliser pour cr√©er un RAID 5, j'ai d'abord d√©mont√© le syst√®me de fichiers mont√© sur `/mnt/raid1` :

```bash
umount /mnt/raid1
```
Ensuite, j'ai arr√™t√© le RAID 1 existant avec la commande suivante : 

```bash
mdadm --stop /dev/md0
```

**Cr√©ation du RAID 5** : 

J'ai ensuite cr√©√© un RAID 5 en utilisant les trois partitions (/dev/sdb1, /dev/sdc1, et /dev/sdd1) avec la commande suivante : 

```bash
mdadm --create /dev/md0 --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1
```
Capture d'√©cran des commandes et sorties de commande :  

![C15_creerraid5](https://github.com/user-attachments/assets/e144cdd1-1aba-4c80-87a5-711d168ec881)

### Formatage et montage du RAID 5

#### Formatage du RAID 5

J'ai format√© le p√©riph√©rique RAID 5 `/dev/md0` avec le syst√®me de fichiers `ext4` et lui ai attribu√© le label "RAID5_Data" :

```bash
mkfs.ext4 /dev/md0 -L "RAID5_Data"
```
**Cr√©ation d'un point de montage**

J'ai cr√©√© un r√©pertoire o√π monter le RAID 5 :

```bash
mkdir /mnt/raid5
```

#### Montage du RAID 5

Le RAID 5 a √©t√© mont√© sur le r√©pertoire /mnt/raid5 avec la commande suivante :  

```bash
mount /dev/md0 /mnt/raid5
```

**V√©rification du montage**

Pour v√©rifier que le RAID 5 a √©t√© mont√© correctement, j'ai utilis√© la commande `df -h`.   

**Capture d'√©cran de toute la manipulation** :   

![C15_end](https://github.com/user-attachments/assets/3b743a9e-a370-4ea8-9653-cb7442f24d6e)   

**V√©rification √©tat du RAID5** : 
# V√©rification de l'√©tat du RAID 5

```bash
mdadm --detail /dev/md0
```
![C16_verifraid5](https://github.com/user-attachments/assets/5fdb115c-36c3-4034-923f-b039deb306d3)


**Voici un historique am√©lior√© des commandes utilis√©es pour le challenge** : 

```bash
# V√©rification des disques disponibles
lsblk

# Cr√©ation du RAID 1 avec deux disques
mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sdb /dev/sdc

# V√©rification de l'√©tat du RAID 1
cat /proc/mdstat
mdadm --detail /dev/md0

# Cr√©ation du fichier de configuration mdadm
mdadm --detail --scan | tee -a /etc/mdadm/mdadm.conf
update-initramfs -u

# Simulation de panne d'un disque
mdadm /dev/md0 --fail /dev/sdb

# V√©rification de l'√©tat apr√®s la simulation de panne
cat /proc/mdstat
mdadm --detail /dev/md0

# Retrait du disque d√©faillant
mdadm /dev/md0 --remove /dev/sdb

# Ajout d'un nouveau disque pour la reconstruction du RAID
mdadm /dev/md0 --add /dev/sdd

# V√©rification de l'√©tat pendant la reconstruction
cat /proc/mdstat
mdadm --detail /dev/md0

# Suppression du RAID 1 pour pr√©parer la cr√©ation du RAID 5
mdadm --stop /dev/md0
mdadm --remove /dev/md0

# Cr√©ation du RAID 5 avec trois disques
mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sdb /dev/sdc /dev/sdd

# V√©rification de l'√©tat du RAID 5
cat /proc/mdstat
mdadm --detail /dev/md0

# Cr√©ation du fichier de configuration mdadm pour le RAID 5
mdadm --detail --scan | tee -a /etc/mdadm/mdadm.conf
update-initramfs -u

# Historique des commandes
history

```
![image](https://github.com/user-attachments/assets/bddb5cce-1cb3-4db8-b332-471118a377cc)

